{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f56db3f-c182-498d-81f0-99104bafb8c0",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "Semantic segmentation, or image segmentation, is the task of clustering parts of an image together which belong to the same object class. In this tutorial, we will...\n",
    "\n",
    "**[Part 1](#Part-1)\n",
    "\n",
    "1. Learn about the [DynamicEarthNet Challenge](#1.-DynamicEarthNet-Challenge),\n",
    "2. Meet the [data](#2.-DynamicEarthNet:-Daily-Multi-Spectral-Satellite-Dataset-for-Semantic-Change-Segmentation),\n",
    "3. Explore a [random forest-based segmentation model](#3.-Random-Forest)\n",
    "4. Design a simple artificial neural network-based solution.\n",
    "\n",
    "**[Part 2](#Part-2)\n",
    "\n",
    "5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4a807-afa2-4883-87ad-4e83e0fd4de7",
   "metadata": {},
   "source": [
    "## 1. DynamicEarthNet Challenge\n",
    "\n",
    "Remote sensing is entering a new era of time-series analysis. Short revisit times of satellites allow for monitoring of many areas across the globe on a weekly basis. However, there has been little exploration of deep learning techniques to leverage this new temporal dimension at scale. Especially, existing approaches have struggled to combine the power of different sensors to make use of all available information. In addition, large scale high quality change detection benchmarks are rare. To stimulate innovation in spatio-temporal machine learning, we have partnered up to propose a unique challenge centered around modeling multi-temporal land cover changes from Planetscope and Sentinel time series data.\n",
    "\n",
    "Do you want to know more? Then, visit the [DynamicEarthNet](https://codalab.lisn.upsaclay.fr/competitions/2882) web page!\n",
    "\n",
    "![Data sample](challenge_sample_cube.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f3621-729e-45c8-bbca-ec478fa8a8d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation\n",
    "\n",
    "**#TODO** Add a description\n",
    "\n",
    "More details can be found in the original [data set paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Toker_DynamicEarthNet_Daily_Multi-Spectral_Satellite_Dataset_for_Semantic_Change_Segmentation_CVPR_2022_paper.pdf).\n",
    "\n",
    "```\n",
    "@inproceedings{toker2022dynamicearthnet,\n",
    "  title={DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation},\n",
    "  author={Toker, Aysim and Kondmann, Lukas and Weber, Mark and Eisenberger, Marvin and Camero, Andr{\\'e}s and Hu, Jingliang and Hoderlein, Ariadna Pregel and {\\c{S}}enaras, {\\c{C}}a{\\u{g}}lar and Davis, Timothy and Cremers, Daniel and others},\n",
    "  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n",
    "  pages={21158--21167},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d8dee-e076-4a47-8da5-2c2d6481a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the required packages using pip install -r requirements.txt\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978154c-eb8e-4670-b3ef-841b52648aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from functools import reduce\n",
    "\n",
    "np.random.seed(3108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ae73b-9310-48be-835c-688316dc729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should match the name of the data sets\n",
    "filename = 'data/dynamic_earth_net.h5'\n",
    "\n",
    "dataset = h5py.File(filename, 'r')\n",
    "\n",
    "# show the content names\n",
    "print(list(dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442102a-4ef2-4578-901e-e53a1f058f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Sentinel-2 data\n",
    "# O'Brien Mountain, California, US. Check it on [Google Maps](https://goo.gl/maps/FwK3my9AX1xZ2Jrs5)\n",
    "sen2_train = np.array(dataset['s2_train'])\n",
    "\n",
    "print(\"Sentinel-2 shape: \", sen2_train.shape)\n",
    "# 24 samples, 1024x1024 pixels, 12 bands\n",
    "\n",
    "def true_color(X):\n",
    "    blue = ((X[:,:,0] - np.min(X[:,:,0])) / (np.max(X[:,:,0]) - np.min(X[:,:,0])))\n",
    "    green = ((X[:,:,1] - np.min(X[:,:,1])) / (np.max(X[:,:,1]) - np.min(X[:,:,1])))\n",
    "    red = ((X[:,:,2] - np.min(X[:,:,2])) / (np.max(X[:,:,2]) - np.min(X[:,:,2])))\n",
    "    return np.dstack((red, green, blue))  \n",
    "\n",
    "index = 2\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12))\n",
    "# B2 = blue, B3 = green, and B4 = red. Note that B1 was not included in the data set\n",
    "plt.imshow(true_color(sen2_train[index]))\n",
    "plt.title('Sentinel-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca195a5f-35a9-4fde-8dc2-faa7eaecfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Planet Fusion data\n",
    "planet_train = np.array(dataset['planet_train'])\n",
    "\n",
    "print(\"Planet Fusion shape: \", planet_train.shape)\n",
    "# 24 samples, 1024x1024 pixels, 4 bands\n",
    "\n",
    "index = 2\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12))\n",
    "# B1 = blue, B2 = green, B3 = red, and B4= NIR\n",
    "plt.imshow(true_color(planet_train[index]))\n",
    "plt.title('Planet Fusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c71131-a934-4331-be4e-0feb1bd6f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ocala area, Florida, US. You can check it on [Google Maps](https://goo.gl/maps/uXmmcdaniRDiQEqA6)\n",
    "sen2_test = np.array(dataset['s2_test'])\n",
    "planet_test = np.array(dataset['planet_test'])\n",
    "\n",
    "fig=plt.figure(figsize=(12, 6))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(true_color(sen2_test[index]))\n",
    "plt.title('Sentinel-2')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(true_color(planet_test[index]))\n",
    "plt.title('Planet Fusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe9202-8022-47b4-80f0-4f3424099aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Planet Fusion data\n",
    "labels_train = np.array(dataset['labels_train'])\n",
    "labels_test = np.array(dataset['labels_test'])\n",
    "\n",
    "print(\"Labels shape: \", labels_train.shape)\n",
    "# 24 samples, 1024x1024 pixels, 12 bands\n",
    "\n",
    "color_map = cm.get_cmap(\"Accent\", lut=labels_train.shape[-1])\n",
    "labels=['impervious surface',\n",
    "       'agriculture',\n",
    "       'forest & other vegetation',\n",
    "       'wetlands',\n",
    "       'soil',\n",
    "       'water',\n",
    "       'snow & ice']\n",
    "\n",
    "def labels2rgb(\n",
    "    X,\n",
    "    colors=color_map.colors):\n",
    "    Z = np.zeros((X.shape[0], X.shape[1], 3))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i,j] = colors[np.argmax(X[i,j]), :3]\n",
    "    return Z\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12))\n",
    "plt.imshow(labels2rgb(labels_train[index]), cmap=color_map)\n",
    "plt.title('Labels')\n",
    "cbar = plt.colorbar(ticks=np.linspace(1/14, 13/14, 7))\n",
    "cbar.set_ticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8f7f1-8710-46b9-a8a6-24a58c0d4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 12))\n",
    "plt.imshow(labels2rgb(labels_test[index]), cmap=color_map)\n",
    "plt.title('Labels')\n",
    "cbar = plt.colorbar(ticks=np.linspace(1/14, 13/14, 7))\n",
    "cbar.set_ticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afcda8-7f50-4ca7-ad51-4737c9445a4a",
   "metadata": {},
   "source": [
    "One key application of our dataset is to measure how a given local region changes over time. For the standard task of binary change detection, we classify each pixel into change or no-change. This definition, however, disregards semantic information. We, therefore, generalize this classical notion to a multi-class segmentation task, which we refer to as semantic change segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da4d14-9aae-40fe-8b04-ce26f8caa209",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(2,6):\n",
    "    fig.add_subplot(2, 2, i-1)\n",
    "    plt.imshow(labels2rgb(labels_train[i]), cmap=color_map)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac2c37-392b-4b59-b3c5-9581655fe0f8",
   "metadata": {},
   "source": [
    "## 3. Random Forest Segmenter\n",
    "\n",
    "Let's start with a very simple approach, namely using a *Random Forest Classifier* to segment an image. Loosely speaking, we will \"classify\" each pixel into one of the semantic segmentation categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc1f82a-e68f-45c8-b842-fc1e716a10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2int(X):\n",
    "    Z = np.zeros(X.shape[:3])    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            for k in range(X.shape[2]):\n",
    "                Z[i,j,k] = np.argmax(X[i,j,k,:]) + 1\n",
    "    return Z\n",
    "\n",
    "labels_train_int = labels2int(labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27070909-c156-44ea-b157-be5602f4c0b1",
   "metadata": {},
   "source": [
    "We now need to format the data so that it fits to what the random forest classifier expects, namely an array of shape (n, m) as input data, and (n) annotations. Note that n corresponds to number of pixels and m to number of features (in our case, bands or channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b71e5-e047-4579-ad59-b08a681b18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(labels_train_int[0], \n",
    "                                   (reduce(lambda x, y: x*y, labels_train_int[0].shape)))\n",
    "print(\"Labels flattened shape\", y.shape)\n",
    "\n",
    "X = np.reshape(planet_train[0], (reduce(lambda x, y: x*y, planet_train[0].shape[:-1]),\n",
    "                                              reduce(lambda x, y: x*y, planet_train[0].shape[-1:])))\n",
    "print(\"Train flattened shape\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff4c6e-ea37-4408-ac34-7f5bed674b1c",
   "metadata": {},
   "source": [
    "Let's train a *Random Forest Classifier* to segment an image. Note that we are training on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8d7f1-5fb9-4ca3-b5f2-a5e37aee8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=3108)\n",
    "rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265063cd-6751-48ac-9d96-53458e33cb31",
   "metadata": {},
   "source": [
    "Now, we will use the trained model to segment a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496aa576-f2ae-496c-96c9-faebe796ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.reshape(planet_train[-1], (reduce(lambda x, y: x*y, planet_train[-1].shape[:-1]),\n",
    "                                              reduce(lambda x, y: x*y, planet_train[-1].shape[-1:])))\n",
    "yz = rfc.predict(Z)\n",
    "yz_img = np.reshape(yz, (labels_train_int[-1].shape[0], labels_train_int[-1].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a24ae-437a-4f78-91b6-b2e0da988f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(labels_train_int[-1], cmap=color_map)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(yz_img, cmap=color_map)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d668209-ddbd-4c8f-a739-a31686cc0e46",
   "metadata": {},
   "source": [
    "On the left side, the ground truth is show, while on the right side the predicted segmentation is shown. Qualitatively, the results are quite OK. So, how about a quantitative appraisal?\n",
    "\n",
    "There are different metrics to evaluate the performance of a semantic segmentation:\n",
    "1. Pixel accuracy: Is the percent of pixels in your image that are classified correctly.\n",
    "2. Intersection-Over-Union (IoU, Jaccard Index): The IoU is the area of overlap between the predicted segmentation and the ground truth divided by the area of union between the predicted segmentation and the ground truth.\n",
    "3. Dice coefficient (F1 Score): Is two times the area of overlap divided by the total number of pixels in both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a95636-7d73-4a5c-9935-ea5ff8b87c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %5.4f\" % metrics.accuracy_score(labels_train_int[1].flatten(), yz))\n",
    "print(\"IoU: %5.4f\" % metrics.jaccard_score(labels_train_int[1].flatten(), yz, average=\"weighted\"))\n",
    "print(\"F1 Score: %5.4f\" % metrics.f1_score(labels_train_int[1].flatten(), yz, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb58876-348e-4d48-bc19-f5e14f09f827",
   "metadata": {},
   "source": [
    "Performance is overall quite good (in-line with the qualitative evaluation). However, it is relatively easy to observe that two classes are *predominant* (aka. majority class). What is happening to individual classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b190362-cdaf-45ca-8a80-56ccfd7deb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.f1_score(labels_train_int[-1].flatten(), yz, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee47ae-4c3c-4d4a-a20f-466ee125dbee",
   "metadata": {},
   "source": [
    "## Do it yourself\n",
    "\n",
    "Train a *Random Forest Classifier* to segment using the test image(s), and compare the performance with respect to the example shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df88d8-e678-42fc-bfcd-b77c2f3fc7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "539b21d3-aaaf-4e74-9dc9-dbeef0c181c1",
   "metadata": {},
   "source": [
    "## 4. Simple CNN-based approach\n",
    "\n",
    "We will implement a FCN (fully convolutional network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555a0cc-fb10-4291-9a6c-14b806833fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN_model(len_classes=7, dropout_rate=0.2):        \n",
    "    input = tf.keras.layers.Input(shape=(1024, 1024, 4))\n",
    "\n",
    "    # A convolution block\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(input)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=1, strides=1)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    # Fully connected layer 2\n",
    "    x = tf.keras.layers.Conv2D(filters=len_classes, kernel_size=1, strides=1)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    # x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "    \n",
    "    predictions = tf.keras.layers.Activation('softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input, outputs=predictions)\n",
    "    \n",
    "    return model   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ea438-7801-4dba-aae6-9d0479f8f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn = FCN_model()\n",
    "print(fcn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efd9eb-b804-4cff-9bae-91d3ae24dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7aeeef-03a7-4636-b2c2-90b5b25ecc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = planet_train[0:3]\n",
    "y = labels_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c3d75-1370-423a-bde8-6590dfe3bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn.fit(X, y, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e838b81-a33e-4136-b520-8d2a987fc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_train[-1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc431e06-7320-432c-a93e-ffa64785aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yz_fcn = fcn.predict(planet_train[-1:])\n",
    "# yz_fcn = np.squeeze(yz_fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7df66-3579-4e21-9b94-dd492f0d7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(labels_train_int[-1], cmap=color_map)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(labels2int(yz_fcn)[0], cmap=color_map)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da3cdb49-30d8-4883-8a24-93de6fee4227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2488\n",
      "IoU: 0.1527\n",
      "F1 Score: 0.2461\n"
     ]
    }
   ],
   "source": [
    "yz_fcn_int = labels2int(yz_fcn)[0]\n",
    "\n",
    "print(\"Accuracy: %5.4f\" % metrics.accuracy_score(labels_train_int[1].flatten(), yz_fcn_int.flatten()))\n",
    "print(\"IoU: %5.4f\" % metrics.jaccard_score(labels_train_int[1].flatten(), yz_fcn_int.flatten(), average=\"weighted\"))\n",
    "print(\"F1 Score: %5.4f\" % metrics.f1_score(labels_train_int[1].flatten(), yz_fcn_int.flatten(), average=\"weighted\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
