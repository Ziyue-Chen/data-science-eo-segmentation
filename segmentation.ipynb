{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f56db3f-c182-498d-81f0-99104bafb8c0",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "Semantic segmentation, or image segmentation, is the task of clustering parts of an image together which belong to the same object class. In this tutorial, we will...\n",
    "\n",
    "## Part 1\n",
    "\n",
    "1. Learn about the [DynamicEarthNet Challenge](#1.-DynamicEarthNet-Challenge),\n",
    "2. Meet the [data](#2.-DynamicEarthNet:-Daily-Multi-Spectral-Satellite-Dataset-for-Semantic-Change-Segmentation),\n",
    "3. Explore a [random forest-based segmentation model](#3.-Random-Forest)\n",
    "4. Design a simple [convolutional neural network-based solution](#4.-Simple-CNN-based-approach)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "5. Use a [U-Net](#5.-The-U-Net) to segment the images,\n",
    "6. Load a [pre-trained models](#6.-Pre-trained-weights), and\n",
    "7. Improve the performance with transfer-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4a807-afa2-4883-87ad-4e83e0fd4de7",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "## 1. DynamicEarthNet Challenge\n",
    "\n",
    "Remote sensing is entering a new era of time-series analysis. Short revisit times of satellites allow for monitoring of many areas across the globe on a weekly basis. However, there has been little exploration of deep learning techniques to leverage this new temporal dimension at scale. Especially, existing approaches have struggled to combine the power of different sensors to make use of all available information. In addition, large scale high quality change detection benchmarks are rare. To stimulate innovation in spatio-temporal machine learning, we have partnered up to propose a unique challenge centered around modeling multi-temporal land cover changes from Planetscope and Sentinel time series data.\n",
    "\n",
    "Do you want to know more? Then, visit the [DynamicEarthNet](https://codalab.lisn.upsaclay.fr/competitions/2882) web page!\n",
    "\n",
    "![Data sample](challenge_sample_cube.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f3621-729e-45c8-bbca-ec478fa8a8d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation\n",
    "\n",
    "**#TODO** Add a description\n",
    "\n",
    "More details can be found in the original [data set paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Toker_DynamicEarthNet_Daily_Multi-Spectral_Satellite_Dataset_for_Semantic_Change_Segmentation_CVPR_2022_paper.pdf).\n",
    "\n",
    "```\n",
    "@inproceedings{toker2022dynamicearthnet,\n",
    "  title={DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation},\n",
    "  author={Toker, Aysim and Kondmann, Lukas and Weber, Mark and Eisenberger, Marvin and Camero, Andr{\\'e}s and Hu, Jingliang and Hoderlein, Ariadna Pregel and {\\c{S}}enaras, {\\c{C}}a{\\u{g}}lar and Davis, Timothy and Cremers, Daniel and others},\n",
    "  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n",
    "  pages={21158--21167},\n",
    "  year={2022}\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d8dee-e076-4a47-8da5-2c2d6481a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the required packages using pip install -r requirements.txt\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978154c-eb8e-4670-b3ef-841b52648aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from functools import reduce\n",
    "\n",
    "np.random.seed(3108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ae73b-9310-48be-835c-688316dc729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should match the name of the data sets\n",
    "filename = 'data/dynamic_earth_net.h5'\n",
    "\n",
    "dataset = h5py.File(filename, 'r')\n",
    "\n",
    "# show the content names\n",
    "print(list(dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442102a-4ef2-4578-901e-e53a1f058f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Sentinel-2 data\n",
    "# O'Brien Mountain, California, US. \n",
    "# Check it on [Google Maps](https://goo.gl/maps/FwK3my9AX1xZ2Jrs5)\n",
    "sen2_train = np.array(dataset['s2_train'])\n",
    "\n",
    "print(\"Sentinel-2 shape: \", sen2_train.shape)\n",
    "# 24 samples, 1024x1024 pixels, 12 bands\n",
    "\n",
    "def true_color(X):\n",
    "    blue = ((X[:,:,0] - np.min(X[:,:,0])) / (np.max(X[:,:,0]) - np.min(X[:,:,0])))\n",
    "    green = ((X[:,:,1] - np.min(X[:,:,1])) / (np.max(X[:,:,1]) - np.min(X[:,:,1])))\n",
    "    red = ((X[:,:,2] - np.min(X[:,:,2])) / (np.max(X[:,:,2]) - np.min(X[:,:,2])))\n",
    "    return np.dstack((red, green, blue))  \n",
    "\n",
    "index = 2\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12))\n",
    "# B2 = blue, B3 = green, and B4 = red. Note that B1 (coastal aerosol) was not\n",
    "# included in the data set. \n",
    "# More info at https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/\n",
    "plt.imshow(true_color(sen2_train[index]))\n",
    "plt.title('Sentinel-2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca195a5f-35a9-4fde-8dc2-faa7eaecfc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Planet Fusion data\n",
    "planet_train = np.array(dataset['planet_train'])\n",
    "\n",
    "print(\"Planet Fusion shape: \", planet_train.shape)\n",
    "# 24 samples, 1024x1024 pixels, 4 bands\n",
    "\n",
    "index = 2\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12))\n",
    "# B1 = blue, B2 = green, B3 = red, and B4= NIR\n",
    "plt.imshow(true_color(planet_train[index]))\n",
    "plt.title('Planet Fusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c71131-a934-4331-be4e-0feb1bd6f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ocala area, Florida, US. You can check it on [Google Maps](https://goo.gl/maps/uXmmcdaniRDiQEqA6)\n",
    "sen2_test = np.array(dataset['s2_test'])\n",
    "planet_test = np.array(dataset['planet_test'])\n",
    "\n",
    "fig=plt.figure(figsize=(12, 6))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(true_color(sen2_test[index]))\n",
    "plt.title('Sentinel-2')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(true_color(planet_test[index]))\n",
    "plt.title('Planet Fusion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe9202-8022-47b4-80f0-4f3424099aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Planet Fusion data\n",
    "labels_train = np.array(dataset['labels_train'])\n",
    "labels_test = np.array(dataset['labels_test'])\n",
    "\n",
    "print(\"Labels shape: \", labels_train.shape)\n",
    "# 24 samples, 1024x1024 pixels, 12 bands\n",
    "\n",
    "color_map = cm.get_cmap(\"Accent\", lut=labels_train.shape[-1])\n",
    "labels=['impervious surface',\n",
    "       'agriculture',\n",
    "       'forest & other vegetation',\n",
    "       'wetlands',\n",
    "       'soil',\n",
    "       'water',\n",
    "       'snow & ice']\n",
    "\n",
    "def labels2rgb(\n",
    "    X,\n",
    "    colors=color_map.colors):\n",
    "    Z = np.zeros((X.shape[0], X.shape[1], 3))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i,j] = colors[np.argmax(X[i,j]), :3]\n",
    "    return Z\n",
    "\n",
    "fig=plt.figure(figsize=(12, 12))\n",
    "plt.imshow(labels2rgb(labels_train[index]), cmap=color_map)\n",
    "plt.title('Labels')\n",
    "cbar = plt.colorbar(ticks=np.linspace(1/14, 13/14, 7))\n",
    "cbar.set_ticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8f7f1-8710-46b9-a8a6-24a58c0d4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 12))\n",
    "plt.imshow(labels2rgb(labels_test[index]), cmap=color_map)\n",
    "plt.title('Labels')\n",
    "cbar = plt.colorbar(ticks=np.linspace(1/14, 13/14, 7))\n",
    "cbar.set_ticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afcda8-7f50-4ca7-ad51-4737c9445a4a",
   "metadata": {},
   "source": [
    "One key application of our dataset is to measure how a given local region changes over time. For the standard task of binary change detection, we classify each pixel into change or no-change. This definition, however, disregards semantic information. We, therefore, generalize this classical notion to a multi-class segmentation task, which we refer to as semantic change segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da4d14-9aae-40fe-8b04-ce26f8caa209",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(2,6):\n",
    "    fig.add_subplot(2, 2, i-1)\n",
    "    plt.imshow(labels2rgb(labels_train[i]), cmap=color_map)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac2c37-392b-4b59-b3c5-9581655fe0f8",
   "metadata": {},
   "source": [
    "## 3. Random Forest Segmenter\n",
    "\n",
    "Let's start with a very simple approach, namely using a *Random Forest Classifier* to segment an image. Loosely speaking, we will \"classify\" each pixel into one of the semantic segmentation categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc1f82a-e68f-45c8-b842-fc1e716a10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels2int(X):\n",
    "    Z = np.zeros(X.shape[:3])    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            for k in range(X.shape[2]):\n",
    "                Z[i,j,k] = np.argmax(X[i,j,k,:]) + 1\n",
    "    return Z\n",
    "\n",
    "labels_train_int = labels2int(labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27070909-c156-44ea-b157-be5602f4c0b1",
   "metadata": {},
   "source": [
    "We now need to format the data so that it fits to what the random forest classifier expects, namely an array of shape (n, m) as input data, and (n) annotations. Note that n corresponds to number of pixels and m to number of features (in our case, bands or channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b71e5-e047-4579-ad59-b08a681b18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(labels_train_int[0], \n",
    "                                   (reduce(lambda x, y: x*y, labels_train_int[0].shape)))\n",
    "print(\"Labels flattened shape\", y.shape)\n",
    "\n",
    "X = np.reshape(planet_train[0], (reduce(lambda x, y: x*y, planet_train[0].shape[:-1]),\n",
    "                                              reduce(lambda x, y: x*y, planet_train[0].shape[-1:])))\n",
    "print(\"Train flattened shape\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff4c6e-ea37-4408-ac34-7f5bed674b1c",
   "metadata": {},
   "source": [
    "Let's train a *Random Forest Classifier* to segment an image. Note that we are training on a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc8d7f1-5fb9-4ca3-b5f2-a5e37aee8794",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=3108)\n",
    "rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265063cd-6751-48ac-9d96-53458e33cb31",
   "metadata": {},
   "source": [
    "Now, we will use the trained model to segment a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496aa576-f2ae-496c-96c9-faebe796ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.reshape(planet_train[-1], (reduce(lambda x, y: x*y, planet_train[-1].shape[:-1]),\n",
    "                                              reduce(lambda x, y: x*y, planet_train[-1].shape[-1:])))\n",
    "yz = rfc.predict(Z)\n",
    "yz_img = np.reshape(yz, (labels_train_int[-1].shape[0], labels_train_int[-1].shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a24ae-437a-4f78-91b6-b2e0da988f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a visualization shortcut, to get the same colors\n",
    "# particularly, we add the missing classes to get the same color map\n",
    "gt = labels_train_int[-1].copy()\n",
    "gt[0,0:len(labels)] = list(range(1,1+len(labels)))\n",
    "yz_img[0,0:len(labels)] = list(range(1,1+len(labels)))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(gt, cmap=color_map)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(yz_img, cmap=color_map)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fdc090-3a22-453d-869a-a968d0386b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = labels_train_int[-1].copy()\n",
    "gt[0,0:len(labels)] = list(range(1,1+len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d668209-ddbd-4c8f-a739-a31686cc0e46",
   "metadata": {},
   "source": [
    "On the left side, the ground truth is show, while on the right side the predicted segmentation is shown. Qualitatively, the results are quite OK. So, how about a quantitative appraisal?\n",
    "\n",
    "There are different metrics to evaluate the performance of a semantic segmentation:\n",
    "1. Pixel accuracy: Is the percent of pixels in your image that are classified correctly.\n",
    "2. Intersection-Over-Union (IoU, Jaccard Index): The IoU is the area of overlap between the predicted segmentation and the ground truth divided by the area of union between the predicted segmentation and the ground truth.\n",
    "3. Dice coefficient (F1 Score): Is two times the area of overlap divided by the total number of pixels in both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a95636-7d73-4a5c-9935-ea5ff8b87c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %5.4f\" % metrics.accuracy_score(labels_train_int[-1].flatten(), yz))\n",
    "print(\"IoU: %5.4f\" % metrics.jaccard_score(labels_train_int[-1].flatten(), yz, average=\"weighted\"))\n",
    "print(\"F1 Score: %5.4f\" % metrics.f1_score(labels_train_int[-1].flatten(), yz, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb58876-348e-4d48-bc19-f5e14f09f827",
   "metadata": {},
   "source": [
    "Performance is overall quite good (in-line with the qualitative evaluation). However, it is relatively easy to observe that two classes are *predominant* (aka. majority class). What is happening to individual classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b190362-cdaf-45ca-8a80-56ccfd7deb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.f1_score(labels_train_int[-1].flatten(), yz, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94105f-4de9-43d4-b758-00c6421a0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we use \"gt\" to get the confusion matrix with all classes\n",
    "cm = metrics.confusion_matrix(gt.flatten(), yz)\n",
    "cm = metrics.confusion_matrix(labels_train_int[-1].flatten(), yz)\n",
    "metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                               display_labels=labels).plot(xticks_rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee47ae-4c3c-4d4a-a20f-466ee125dbee",
   "metadata": {},
   "source": [
    "## Do it yourself\n",
    "\n",
    "Train a *Random Forest Classifier* to segment using the test image(s), and compare the performance with respect to the example shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df88d8-e678-42fc-bfcd-b77c2f3fc7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "539b21d3-aaaf-4e74-9dc9-dbeef0c181c1",
   "metadata": {},
   "source": [
    "## 4. Simple CNN-based approach\n",
    "\n",
    "We will implement a FCN (fully convolutional network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7555a0cc-fb10-4291-9a6c-14b806833fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn_model(num_classes=7, dropout_rate=0.2):        \n",
    "    input = tf.keras.layers.Input(shape=(1024, 1024, 4))\n",
    "\n",
    "    # A convolution block\n",
    "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\")(input)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    # Fully connected layer 1\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=1, strides=1)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    \n",
    "    # Fully connected layer 2\n",
    "    x = tf.keras.layers.Conv2D(filters=num_classes, kernel_size=1, strides=1)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)    \n",
    "    \n",
    "    predictions = tf.keras.layers.Activation('softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=input, outputs=predictions)\n",
    "    \n",
    "    return model   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ea438-7801-4dba-aae6-9d0479f8f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn = fcn_model()\n",
    "print(fcn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efd9eb-b804-4cff-9bae-91d3ae24dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7aeeef-03a7-4636-b2c2-90b5b25ecc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fcn = planet_train[0:1]\n",
    "y_fcn = labels_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c3d75-1370-423a-bde8-6590dfe3bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn.fit(X_fcn, y_fcn, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc431e06-7320-432c-a93e-ffa64785aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yz_fcn = fcn.predict(planet_train[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7df66-3579-4e21-9b94-dd492f0d7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yz_fcn_int = labels2int(yz_fcn)[0]\n",
    "yz_fcn_int[0,0:len(labels)] = list(range(1,1+len(labels)))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(gt, cmap=color_map)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(yz_fcn_int, cmap=color_map)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3cdb49-30d8-4883-8a24-93de6fee4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %5.4f\" % \n",
    "      metrics.accuracy_score(labels_train_int[-1].flatten(), yz_fcn_int.flatten()))\n",
    "print(\"IoU: %5.4f\" % \n",
    "      metrics.jaccard_score(labels_train_int[-1].flatten(), yz_fcn_int.flatten(), average=\"weighted\"))\n",
    "print(\"F1 Score: %5.4f\" % \n",
    "      metrics.f1_score(labels_train_int[-1].flatten(), yz_fcn_int.flatten(), average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c58582-3a8e-4cc4-a7cb-3cd6139c941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we use \"gt\" to get the confusion matrix with all classes\n",
    "cm_fcn = metrics.confusion_matrix(gt.flatten(), yz_fcn_int.flatten())\n",
    "metrics.ConfusionMatrixDisplay(confusion_matrix=cm_fcn, \n",
    "                               display_labels=labels).plot(xticks_rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372e308-1fe7-4e7c-879c-3a17df2bd7a3",
   "metadata": {},
   "source": [
    "## Do it yourself\n",
    "\n",
    "Notice that the input's range differs from one channel to the other (check the `code` block below). Then, standarize or normalize the input, and try the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641ab6d9-b0d8-484b-8817-06f06e6aea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_fcn[:,:,:,2].flatten(), label=\"red\", color=\"red\", alpha=0.8)\n",
    "plt.hist(X_fcn[:,:,:,1].flatten(), label=\"green\", color=\"green\", alpha=0.8)\n",
    "plt.hist(X_fcn[:,:,:,0].flatten(), label=\"blue\", color=\"blue\", alpha=0.8)\n",
    "plt.xlabel(\"Pixel value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090a0a7-ad7a-4f5d-8df1-9b57679503e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91a126ab-e3a7-4af9-8b28-a127acaadc98",
   "metadata": {},
   "source": [
    "## Do it yourself\n",
    "\n",
    "Define a new model based on the `fcn_model`, but this time add a new layer. Also, try a different activation function and modify the number of filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006c53b-d65d-47c7-8694-df2e9fd7beac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49edadb4-c29c-4339-9c85-fc62a50532bd",
   "metadata": {},
   "source": [
    "## Do it yourself\n",
    "\n",
    "Train your new model on the whole training data set, and evaluate the performance of the model on the first image of the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59f299-eb28-4848-8e95-9671f4853c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "995a1369-6cf3-4206-909b-912ea509d8cb",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "## 5. The U-Net\n",
    "\n",
    "The U-Net is convolutional network architecture for fast and precise segmentation of images. It was originally invented, and first used for biomedical image segmentation. Its architecture can be broadly seen as an encoder network followed by a decoder network. Unlike classification, where the end result of the the deep network is the only important thing, semantic segmentation not only requires discrimination at pixel level but also a mechanism to project the discriminative features learnt at different stages of the encoder onto the pixel space.\n",
    "\n",
    "The *encoder* is usually a pre-trained classification network (e.g., VGG or ResNet) where convolution blocks followed by maxpool downsampling are applied to encode the input image into multiple feature representation spaces.\n",
    "\n",
    "The *decoder* semantically project the discriminative features (lower resolution) learnt by the encoder onto the pixel space (i.e., higher resolution) to get a dense classification (aka. segmentation).\n",
    "\n",
    "The following image introduces the U-Net architecture (image taken from [*U-Net: Convolutional Networks for Biomedical Image Segmentation*](https://arxiv.org/abs/1505.04597v1))\n",
    "\n",
    "![U-Net architecture](u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158e63e-5848-4394-840e-28cc814a4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(num_classes=7,\n",
    "               input_size=(1024, 1024, 4),\n",
    "               drop_out=0.5):\n",
    "    # encoder\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "    # maybe try adding kernel_initializer='he_normal'\n",
    "    conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    drop4 = tf.keras.layers.Dropout(drop_out)(conv4)\n",
    "    pool4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = tf.keras.layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5 = tf.keras.layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    drop5 = tf.keras.layers.Dropout(drop_out)(conv5)\n",
    "\n",
    "    # decoder\n",
    "    up6 = tf.keras.layers.Conv2D(512, 2, activation='relu', padding='same')(\n",
    "        tf.keras.layers.UpSampling2D(size=(2,2))(drop5))\n",
    "    merge6 = tf.keras.layers.concatenate([drop4, up6], axis=3)\n",
    "    conv6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(merge6)\n",
    "    conv6 = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = tf.keras.layers.Conv2D(256, 2, activation='relu', padding='same')(\n",
    "        tf.keras.layers.UpSampling2D(size=(2,2))(conv6))\n",
    "    merge7 = tf.keras.layers.concatenate([conv3, up7], axis = 3)\n",
    "    conv7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(merge7)\n",
    "    conv7 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    up8 = tf.keras.layers.Conv2D(128, 2, activation='relu', padding='same')(\n",
    "        tf.keras.layers.UpSampling2D(size=(2,2))(conv7))\n",
    "    merge8 = tf.keras.layers.concatenate([conv2, up8], axis=3)\n",
    "    conv8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(merge8)\n",
    "    conv8 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)\n",
    "\n",
    "    up9 = tf.keras.layers.Conv2D(64, 2, activation='relu', padding='same')(\n",
    "        tf.keras.layers.UpSampling2D(size=(2,2))(conv8))\n",
    "    merge9 = tf.keras.layers.concatenate([conv1, up9], axis=3)\n",
    "    conv9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(merge9)\n",
    "    conv9 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)\n",
    "    conv9 = tf.keras.layers.Conv2D(2, 3, activation='relu', padding='same')(conv9)\n",
    "    \n",
    "    conv10 = tf.keras.layers.Conv2D(num_classes, 1, activation='relu')(conv9)\n",
    "    \n",
    "    predictions = tf.keras.layers.Activation('sigmoid')(conv10)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions)    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c85c4fe-efec-4118-9027-5950e8155c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = unet_model()\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e73755-d129-4754-a765-e8ec93e8c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b851ed-9a2e-4e37-9d14-1cf01195c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.fit(X_fcn, y_fcn, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836ae07-bf95-445e-84dc-d2c2abf710ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "yz_unet = unet.predict(planet_train[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8ca43-1f4b-44e7-8043-ff2f9618a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "yz_unet_int = labels2int(yz_unet)[0]\n",
    "yz_unet_int[0,0:len(labels)] = list(range(1,1+len(labels)))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(gt, cmap=color_map)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(yz_unet_int, cmap=color_map)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b1d117-d097-4730-bb31-a22e11b7a9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %5.4f\" % \n",
    "      metrics.accuracy_score(labels_train_int[-1].flatten(), yz_unet_int.flatten()))\n",
    "print(\"IoU: %5.4f\" % \n",
    "      metrics.jaccard_score(labels_train_int[-1].flatten(), yz_unet_int.flatten(), average=\"weighted\"))\n",
    "print(\"F1 Score: %5.4f\" % \n",
    "      metrics.f1_score(labels_train_int[-1].flatten(), yz_unet_int.flatten(), average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66e9f6-f3c5-4d1b-a5ce-9ca4570abcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we use \"gt\" to get the confusion matrix with all classes\n",
    "cm_unet = metrics.confusion_matrix(gt.flatten(), yz_unet_int.flatten())\n",
    "metrics.ConfusionMatrixDisplay(confusion_matrix=cm_unet, \n",
    "                               display_labels=labels).plot(xticks_rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe91da-fbd2-44b8-8812-100bf01391d3",
   "metadata": {},
   "source": [
    "## Do it yourself\n",
    "\n",
    "Train the model using the standarized or normalized input data. Try to increase the training budget (i.e., the number of epochs), and adjust the *drop out*. Do you see any improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce26785-e9f7-46d7-9cdb-7770c0391dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65e8e645-b622-4f9c-a1cd-a7897d67df55",
   "metadata": {},
   "source": [
    "## 6. Pre-trained weights\n",
    "\n",
    "You may have notice that the U-Net architecture (as define above) has more than 31 million trainable parameters! Therefore, an immense amount of data is needed to train the network (if you want to get a decent performance).\n",
    "\n",
    "Since we don't have much data, and we want to save time, let's try to reuse a pre-trained model.\n",
    "\n",
    "What is a pre-trained model? In plain English, it is a model that was trained using a data set, and its weights have been saved. Usually, the data set used to train is *general*, e.g., imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e5112-298b-4eee-bf82-5746791974a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_unet_model(input_size=(1024, 1024, 4)):\n",
    "    # note that we are fixing the number of channels, because the pre-trained model\n",
    "    # was designed and trained for RGB\n",
    "    inputs = tf.keras.layers.Input(shape=(input_size[0], input_size[1], 3), name=\"input_image\")\n",
    "    \n",
    "    encoder = tf.keras.applications.MobileNetV2(\n",
    "        input_tensor=inputs, \n",
    "        weights=\"imagenet\", \n",
    "        include_top=False, \n",
    "        alpha=0.35)\n",
    "    \n",
    "    skip_connection_names = [\n",
    "        \"input_image\", \n",
    "        \"block_1_expand_relu\", \n",
    "        \"block_3_expand_relu\", \n",
    "        \"block_6_expand_relu\"]\n",
    "    \n",
    "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
    "    \n",
    "    f = [16, 32, 48, 64]\n",
    "    x = encoder_output\n",
    "    for i in range(1, len(skip_connection_names)+1, 1):\n",
    "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
    "        x = tf.keras.layers.UpSampling2D((2, 2))(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, x_skip])\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(\"relu\")(x)\n",
    "        \n",
    "    x = tf.keras.layers.Conv2D(7, (1, 1), padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, x)\n",
    "    \n",
    "    if input_size[-1] != 3:\n",
    "        # we need to adjust the model... there are several alternatives,\n",
    "        # let's add a Conv2D to reshape the input\n",
    "        full_inputs = tf.keras.layers.Input(shape=input_size)\n",
    "        z = tf.keras.layers.Conv2D(3, 3, padding='same', name=\"input_image\")(full_inputs)        \n",
    "        z = model(z)\n",
    "        # let's redefine the model\n",
    "        model = tf.keras.Model(full_inputs, z)        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60714db-008f-48bc-a9c5-ce41191f58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_unet = pre_unet_model()\n",
    "pre_unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50b26c-fb68-41d2-87e9-e04eccf04f38",
   "metadata": {},
   "source": [
    "Let's check the performance of the pre-trained model *out-of-the-box*, i.e., even without doing a fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b88e52-49c0-44e1-bbf3-2413cb838e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "yz_pre_unet = pre_unet.predict(planet_train[-1:])\n",
    "\n",
    "yz_pre_unet_int = labels2int(yz_pre_unet)[0]\n",
    "yz_pre_unet_int[0,0:len(labels)] = list(range(1,1+len(labels)))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(gt, cmap=color_map)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(yz_pre_unet_int, cmap=color_map)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a45f6cb-5b3e-4c38-a858-d0c3226f53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %5.4f\" % \n",
    "      metrics.accuracy_score(labels_train_int[-1].flatten(), yz_pre_unet_int.flatten()))\n",
    "print(\"IoU: %5.4f\" % \n",
    "      metrics.jaccard_score(labels_train_int[-1].flatten(), yz_pre_unet_int.flatten(), average=\"weighted\"))\n",
    "print(\"F1 Score: %5.4f\" % \n",
    "      metrics.f1_score(labels_train_int[-1].flatten(), yz_pre_unet_int.flatten(), average=\"weighted\"))\n",
    "\n",
    "# Note that we use \"gt\" to get the confusion matrix with all classes\n",
    "cm_pre_unet = metrics.confusion_matrix(gt.flatten(), yz_pre_unet_int.flatten())\n",
    "metrics.ConfusionMatrixDisplay(confusion_matrix=cm_pre_unet, \n",
    "                               display_labels=labels).plot(xticks_rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ee63a-115e-417a-ac55-78b98cccef6f",
   "metadata": {},
   "source": [
    "## Do it yourself\n",
    "\n",
    "Try a different pre-trained model. Check the [keras documentation](https://keras.io/api/applications/), and select your favorite alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63cb779-d2e1-4e7f-a124-70639ac44100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d88fbdb-02b8-4383-b307-e3f06fd8c736",
   "metadata": {},
   "source": [
    "## 7. Transfer learning\n",
    "\n",
    "You learnt to load a pre-trained model, and most likely, you notice that you can benefit *out-of-the-box* of these models. However, most of the times, the pre-trained models were trained on completely different problems. Therefore, it makes sense to adapt the behavior of our model to the problem, aka. [transfer-learning](https://keras.io/guides/transfer_learning/).\n",
    "\n",
    "Let's start with a simple approach. We will add a simple classifier on top of the model, and we will train it to *convert* the output of the pre-trained model to something new. In the meantime, we will *freeze* the pre-trained weights (i.e., the weights will not be trainable). Note that we are only freezing the pre-trained weights, the first layer (i.e., the Conv2D in the case of 4 bands) is still to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d414d2-5a96-4c4c-899a-5044f954ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_unet.layers[2].trainable = False\n",
    "pre_unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9c26e-b2ff-4284-a093-bf7af6156c2e",
   "metadata": {},
   "source": [
    "Let's add a simple model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfe5e5a-5452-4f27-91bd-c575600cbff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(1024, 1024, 4))\n",
    "x = pre_unet(inputs)\n",
    "x = tf.keras.layers.Conv2D(7, (1, 1), padding=\"same\")(x)\n",
    "x = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "transfer_model = tf.keras.Model(inputs, x)\n",
    "\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5825b-3454-455f-97db-3d60b823cbc8",
   "metadata": {},
   "source": [
    "Notice that the number of parameters to train reduced dramatically.\n",
    "\n",
    "Now, let's train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757c919-2f5c-4eee-8c52-3bc1acd2288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "transfer_model.fit(X_fcn, y_fcn, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7d6e5-7af7-4fb0-944a-0bc6fa6bee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yz_transfer_model = transfer_model.predict(planet_train[-1:])\n",
    "\n",
    "yz_transfer_model_int = labels2int(yz_transfer_model)[0]\n",
    "yz_transfer_model_int[0,0:len(labels)] = list(range(1,1+len(labels)))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(gt, cmap=color_map)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(yz_transfer_model_int, cmap=color_map)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ab3dc-78a4-4ee0-b536-67ba2eb9fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %5.4f\" % \n",
    "      metrics.accuracy_score(labels_train_int[-1].flatten(), yz_transfer_model_int.flatten()))\n",
    "print(\"IoU: %5.4f\" % \n",
    "      metrics.jaccard_score(labels_train_int[-1].flatten(), yz_transfer_model_int.flatten(), average=\"weighted\"))\n",
    "print(\"F1 Score: %5.4f\" % \n",
    "      metrics.f1_score(labels_train_int[-1].flatten(), yz_transfer_model_int.flatten(), average=\"weighted\"))\n",
    "\n",
    "# Note that we use \"gt\" to get the confusion matrix with all classes\n",
    "cm_transfer_model = metrics.confusion_matrix(gt.flatten(), yz_transfer_model_int.flatten())\n",
    "metrics.ConfusionMatrixDisplay(confusion_matrix=cm_transfer_model, \n",
    "                               display_labels=labels).plot(xticks_rotation=\"vertical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d237fa2-3a22-44d4-bb32-7a5b717e2d44",
   "metadata": {},
   "source": [
    "## Do it yourself\n",
    "\n",
    "What happens if we get rid of the NIR band, and use RGB instead? Repeat the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d06760-af17-48c3-a07a-0c5f1efecaae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
